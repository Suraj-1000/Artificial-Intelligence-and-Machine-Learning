{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_YuCxmeI-g5"
      },
      "source": [
        "\n",
        "```\n",
        "Student ID: 2357572\n",
        "Student Name: Suraj Kanwar\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "V7DGbuYtB1JI",
        "outputId": "919f30d9-eb40-41e1-b204-01b13bb904ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxV-QBHp-B6J"
      },
      "source": [
        "## Helper Function for Text Cleaning:\n",
        "\n",
        "Implement a Helper Function as per Text Preprocessing Notebook and Complete the following pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ggrjx6HiJQRz"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MqVARJCRJbhU",
        "outputId": "a6d7b5aa-4b94-45b6-f7ca-c7df925f8e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-llg-TI_Drw"
      },
      "source": [
        "# Build a Text Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KCT1T3-68tN9"
      },
      "outputs": [],
      "source": [
        "# Text Cleaning Pipeline\n",
        "def text_cleaning_pipeline(dataset, rule=\"lemmatize\"):\n",
        "    \"\"\"\n",
        "    Preprocesses a single text string by cleaning, tokenizing, removing stopwords,\n",
        "    and applying lemmatization or stemming.\n",
        "\n",
        "    Args:\n",
        "        dataset (str): Input text to be cleaned.\n",
        "        rule (str): Either 'lemmatize' or 'stem' to choose the normalization method.\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned and processed text as a single string.\n",
        "    \"\"\"\n",
        "    if not isinstance(dataset, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    data = dataset.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    data = re.sub(r'https?://\\S+|www\\.\\S+', '', data)\n",
        "\n",
        "    # Remove emojis\n",
        "    data = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+', '', data)\n",
        "\n",
        "    # Remove unwanted characters (keep alphanumeric, spaces, and basic punctuation)\n",
        "    data = re.sub(r'[^a-z0-9\\s.,!?]', '', data)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(data)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Apply lemmatization or stemming\n",
        "    if rule == \"lemmatize\":\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    elif rule == \"stem\":\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "    else:\n",
        "        print(\"Pick between lemmatize or stem\")\n",
        "        return \"\"\n",
        "\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzMm4-1KCNkH"
      },
      "source": [
        "# Text Classification using Machine Learning Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFltIxr9L2Wu"
      },
      "source": [
        "### üìù Instructions: Trump Tweet Sentiment Classification\n",
        "\n",
        "1. **Load the Dataset**  \n",
        "   Load the dataset named `\"trump_tweet_sentiment_analysis.csv\"` using `pandas`. Ensure the dataset contains at least two columns: `\"text\"` and `\"label\"`.\n",
        "\n",
        "2. **Text Cleaning and Tokenization**  \n",
        "   Apply a text preprocessing pipeline to the `\"text\"` column. This should include:\n",
        "   - Lowercasing the text  \n",
        "   - Removing URLs, mentions, punctuation, and special characters  \n",
        "   - Removing stopwords  \n",
        "   - Tokenization (optional: stemming or lemmatization)\n",
        "   - \"Complete the above function\"\n",
        "\n",
        "3. **Train-Test Split**  \n",
        "   Split the cleaned and tokenized dataset into **training** and **testing** sets using `train_test_split` from `sklearn.model_selection`.\n",
        "\n",
        "4. **TF-IDF Vectorization**  \n",
        "   Import and use the `TfidfVectorizer` from `sklearn.feature_extraction.text` to transform the training and testing texts into numerical feature vectors.\n",
        "\n",
        "5. **Model Training and Evaluation**  \n",
        "   Import **Logistic Regression** (or any machine learning model of your choice) from `sklearn.linear_model`. Train it on the TF-IDF-embedded training data, then evaluate it using the test set.  \n",
        "   - Print the **classification report** using `classification_report` from `sklearn.metrics`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TWKl1d5pJpst",
        "outputId": "a1c083e5-50af-4409-c4cd-cc961b5eea1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'Sentiment'], dtype='object')\n",
            "                                                text  Sentiment\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...          0\n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...          0\n",
            "2  Trump protests: LGBTQ rally in New York https:...          1\n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...          0\n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...          0\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Artificial Intelligence and Machine Learning/Week:8/trum_tweet_sentiment_analysis.csv')\n",
        "\n",
        "# Verify the columns\n",
        "print(data.columns)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BoYBEiU0J6mR",
        "outputId": "a7f239de-1da3-469b-f9a5-e33512b6e2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...   \n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...   \n",
            "2  Trump protests: LGBTQ rally in New York https:...   \n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...   \n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  rt johnleguizamo trump draining swamp taxpayer...  \n",
            "1  icymi hacker rig fm radio station play antitru...  \n",
            "2  trump protest lgbtq rally new york bbcworld vi...  \n",
            "3  hi im pier morgan . david beckham awful donald...  \n",
            "4  rt glennfranco68 tech firm suing buzzfeed publ...  \n"
          ]
        }
      ],
      "source": [
        "# Step 2: Text Cleaning and Tokenization\n",
        "# Drop rows with missing 'text' values\n",
        "data = data.dropna(subset=['text'])\n",
        "\n",
        "# Apply the text cleaning pipeline\n",
        "data['cleaned_text'] = data['text'].apply(lambda x: text_cleaning_pipeline(x, rule=\"lemmatize\"))\n",
        "\n",
        "# Verify the cleaned text\n",
        "print(data[['text', 'cleaned_text']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yUnbZez_KA3C",
        "outputId": "19d31612-dbd6-4a90-d7aa-45de973f8fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1480098\n",
            "Testing set size: 370025\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Train-Test Split\n",
        "# Define features (cleaned text) and labels (correct column name: 'Sentiment')\n",
        "X = data['cleaned_text']\n",
        "y = data['Sentiment']  # Corrected from 'label' to 'Sentiment'\n",
        "\n",
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the split\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KcUP7q5oMp_v",
        "outputId": "286fd7d0-223a-4108-86d6-402368f45bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training TF-IDF shape: (1480098, 5000)\n",
            "Testing TF-IDF shape: (370025, 5000)\n"
          ]
        }
      ],
      "source": [
        "# Step 4: TF-IDF Vectorization\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Verify the shape of the TF-IDF matrices\n",
        "print(f\"Training TF-IDF shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Testing TF-IDF shape: {X_test_tfidf.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Tbxjl4XpMzxv",
        "outputId": "a0545c32-54db-4233-e518-115d4e317ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94    248563\n",
            "           1       0.90      0.85      0.87    121462\n",
            "\n",
            "    accuracy                           0.92    370025\n",
            "   macro avg       0.91      0.90      0.90    370025\n",
            "weighted avg       0.92      0.92      0.92    370025\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Model Training and Evaluation\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}